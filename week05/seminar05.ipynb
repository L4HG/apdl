{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiheadAttention in details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(2 * 3 * 4 * 5).reshape(2, 3, 4, 5)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's swap second and third dimensions and then union first and second dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x.transpose(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(torch.cat([z_ for z_ in z]), z.flatten(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2, 3, 4, 5] -> [2, 4, 3, 5] -> [8, 3, 5]\n",
    "x.transpose(1, 2).flatten(0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.allclose(\n",
    "    einops.rearrange(x, 'first second third fourth -> (first third) second fourth'),\n",
    "    x.transpose(1, 2).flatten(0, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is more readable? :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim: int, num_heads: int, dropout: float):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.head_dim = input_dim // num_heads\n",
    "        \n",
    "        assert self.head_dim * num_heads == self.input_dim\n",
    "        \n",
    "        self.scaling = self.head_dim ** -0.5\n",
    "        \n",
    "        # Gather Q, K, V projections into one big projection\n",
    "        self.projection = nn.Linear(input_dim, input_dim * 3, bias=False)\n",
    "        self.out_projection = nn.Linear(input_dim, input_dim, bias=False)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_key_padding_mask(lengths: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            lengths (torch.Tensor):\n",
    "        Returns: mask to exclude keys that are pads, of shape `(batch, src_len)`,\n",
    "            where padding elements are indicated by 1s.\n",
    "        \"\"\"\n",
    "        \n",
    "        max_length = torch.max(lengths).item()\n",
    "        mask = (\n",
    "            torch.arange(max_length, device=lengths.device)\n",
    "            .ge(lengths.view(-1, 1))\n",
    "            .contiguous()\n",
    "            .bool()\n",
    "        )\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def _check_input_shape(self, input: torch.Tensor, mask: torch.BoolTensor):\n",
    "        if input.dim() != 3:\n",
    "            raise ValueError('Input should have 3 dimensions')\n",
    "\n",
    "        if input.size(-1) != self.input_dim:\n",
    "            raise ValueError('Expected order of dimensions is [T, B, C]')\n",
    "\n",
    "        if mask.dtype != torch.bool:\n",
    "            raise ValueError('Expected type of mask is torch.bool')\n",
    "    \n",
    "    def forward(self, input: torch.Tensor, key_padding_mask: torch.BoolTensor) -> torch.Tensor:\n",
    "        self._check_input_shape(input, key_padding_mask)\n",
    "\n",
    "        input_len, batch_size, _ = input.size()\n",
    "\n",
    "        query, key, value = self.projection(input).chunk(3, dim=-1)\n",
    "        assert query.size() == (input_len, batch_size, self.input_dim)\n",
    "        \n",
    "        # Gather batches with heads\n",
    "        query = einops.rearrange(\n",
    "            query, 'T batch (head dim) -> (batch head) T dim', head=self.num_heads\n",
    "        )\n",
    "        key = einops.rearrange(\n",
    "            key, 'T batch (head dim) -> (batch head) dim T', head=self.num_heads\n",
    "        )\n",
    "        value = einops.rearrange(\n",
    "            value, 'T batch (head dim) -> (batch head) T dim', head=self.num_heads\n",
    "        )\n",
    "\n",
    "        attn_weights = torch.bmm(query, key)\n",
    "        attn_weights.mul_(self.scaling)\n",
    "        assert attn_weights.size() == (batch_size * self.num_heads, input_len, input_len)\n",
    "\n",
    "        # Masking padding scores\n",
    "        attn_weights = attn_weights.view(batch_size, self.num_heads, input_len, input_len)\n",
    "        attn_weights = attn_weights.masked_fill(\n",
    "            key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "            float('-inf'),\n",
    "        )\n",
    "        attn_weights = attn_weights.view(batch_size * self.num_heads, input_len, input_len)\n",
    "\n",
    "        attn_probs = torch.softmax(attn_weights, dim=-1)\n",
    "        attn_probs = self.dropout(attn_probs)\n",
    "\n",
    "        attn = torch.bmm(attn_probs, value)\n",
    "        assert attn.size() == (batch_size * self.num_heads, input_len, self.head_dim)\n",
    "\n",
    "        attn = einops.rearrange(\n",
    "            attn, '(batch head) T dim -> T batch (head dim)', head=self.num_heads\n",
    "        )\n",
    "        attn = self.out_projection(attn)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        return attn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformers in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.Transformer\n",
    "nn.TransformerDecoder\n",
    "nn.TransformerDecoderLayer\n",
    "nn.TransformerEncoder\n",
    "nn.TransformerEncoderLayer\n",
    "nn.MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to train Transformers\n",
    "\n",
    "https://tnq177.github.io/data/transformers_without_tears.pdf\n",
    "\n",
    "https://arxiv.org/pdf/1804.00247.pdf\n",
    "\n",
    "https://tunz.kr/post/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
